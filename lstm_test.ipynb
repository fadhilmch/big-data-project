{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name,\n",
    "          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50):\n",
    "    # setup data and models\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "    m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocabulary,\n",
    "              num_layers=num_layers)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    orig_decay = lr_decay\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        sess.run([init_op])\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            # m.assign_lr(sess, learning_rate)\n",
    "            # print(m.learning_rate.eval(), new_lr_decay)\n",
    "            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "            curr_time = dt.datetime.now()\n",
    "            for step in range(training_input.epoch_size):\n",
    "                # cost, _ = sess.run([m.cost, m.optimizer])\n",
    "                if step % print_iter != 0:\n",
    "                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state],\n",
    "                                                      feed_dict={m.init_state: current_state})\n",
    "                else:\n",
    "                    seconds = (float((dt.datetime.now() - curr_time).seconds) / print_iter)\n",
    "                    curr_time = dt.datetime.now()\n",
    "                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],\n",
    "                                                           feed_dict={m.init_state: current_state})\n",
    "                    print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}, Seconds per step: {:.3f}\".format(epoch,\n",
    "                            step, cost, acc, seconds))\n",
    "\n",
    "            # save a model checkpoint\n",
    "            saver.save(sess, data_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "        # do a final save\n",
    "        saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path ='./data/cleaned_train_red.csv'\n",
    "    valid_path = './data/cleaned_val.csv'\n",
    "    test_path = './data/cleaned_test.csv'\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary\n",
    "\n",
    "\n",
    "def batch_producer(raw_data, batch_size, num_steps):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = data[:, i * num_steps:(i + 1) * num_steps]\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class Input(object):\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)\n",
    "\n",
    "\n",
    "# create the main model\n",
    "class Model(object):\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,\n",
    "                 dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # create the word embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n",
    "\n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs, dropout)\n",
    "\n",
    "        # set up the state storage / extraction\n",
    "        self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])\n",
    "\n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "        rnn_tuple_state = tuple(\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "             for idx in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # create an LSTM cell to be unrolled\n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "        # add a dropout wrapper if training\n",
    "        if is_training and dropout < 1:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\n",
    "\n",
    "        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)\n",
    "        # reshape to (batch_size * num_steps, hidden_size)\n",
    "        output = tf.reshape(output, [-1, hidden_size])\n",
    "\n",
    "        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.input_obj.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "\n",
    "        # Update the cost\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # get the prediction accuracy\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        if not is_training:\n",
    "           return\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        # self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/cleaned_train.csv', encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103625, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = dataset[(dataset.target==1)]\n",
    "neg = dataset[(dataset.target==0)]\n",
    "\n",
    "train_dataset = pd.concat([pos[1:230000], neg[1:230000]], ignore_index=True)\n",
    "valid_dataset = pd.concat([pos[230000:-1], neg[230000:-1]], ignore_index=True)\n",
    "valid_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset.to_csv('data/cleaned_val.csv', index=False)\n",
    "train_dataset.to_csv('data/cleaned_train_red.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
